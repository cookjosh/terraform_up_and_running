# Overall Notes
This file is the overall notes from my progression through the book, which was originally [this series of blogposts](https://blog.gruntwork.io/a-comprehensive-guide-to-terraform-b3d32832baca#.b6sun4nkn). I've linked the blogpost throughout for relevant images as well. For each chapter, I broke out the individual chapter notes into their own file in their respective chapter folder in the repo.

## Chapter 2 - Getting Started with Terraform
- resources are defined with the convention of `resource` `<provider_resource>` `<name>` where `provider` refers to the backend provider (in this case AWS), `resource` refers to a provider's product (eg EC2 instance), and `name` which can be referenced by other parts of the code.
- `user_data_replace_on_change = true` - This parameter terminates the original instance and launches a replacement. This is because terraform will typically only update the original in place but `user_data` is only run on initial boot!
- AWS does not allow traffic to/from an instance by default, so a security group is required.
    - For an `instance` to use a sg, it needs a `vpc_security_group_ids` argument.
- Terraform refers to `expressions` as anything that returns a value.
    - `Literals` are the simplest type (strings and numbers)
    - `references` can be used to refer to other parts of the code, like a `resource attribute reference` which uses syntax like `<provider>_<type>.<name>.<attribute>`
        - Remember that resources have `<name>` in their definition syntax that can be referred to elsewhere (like by a `resource attribute reference`)
        - `<attribute>` can be an argument (eg name) or something "exported" by the resource (eg `id` in the case of security groups).
- References between resources creates "implicit dependencies" on each other, so terraform creates a dependency graph to determine in what order to create resources (if all applied from scratch) or in parallel if possible.
- Like in other languages, we can assign values to variables to be used throughout our code and minimize repetitiveness. Variables are delcared using syntax like `variable "NAME" { [CONFIG ...] }`
    - The body of the variable declaration (in the config brackets) can contain:
        - `description` - can be used as documentation in code but also is output to terminal when running `plan` or `apply`.
        - `default` - a way to set a "fallback" value for the variable.
        - `type` - set constraints for what can be passed in.
        - `validation` - custom input validation rules (eg char length)
        - `sensitive` - if `true`, terraform won't log it when you run `plan` or `apply`, so useful for things like secrets.
    - After declaring a variable, we can reference it with the syntax `var.<VARIABLE_NAME>` and use this in place of hardcoding the same value in multiple places!
    - They can also be used in a string literal (like our `user_data` script) using an `interpolation` with the syntax of `${...}`.
- We can not only create input variables as above but also `output` variables using syntax like `output "<NAME>" { value = <VALUE> [CONFIG...]}`.
    - `name` is how we can reference it elsewhere in the code.
    - `value` can be any terraform expression to be outputted.
    - `config` can contain the parameters:
        - `description` - similar to how it's used in `variable` parameterss
        - `sensitive` - similar to `variable`
        - `depends_on` - a way to tell terraform there is a dependency between resources that it might not pick up through its own dependency graph.
    - outputs can be outputted without `plan` or `apply` by using `terraform output`.
- AWS Auto Scaling Group (ASG) is used to manage the size of the cluster based on factors like individual server health, traffic, etc...
    - They require a `launch configuration` which is declared similar to an `aws_instance` resource, but does use slightly different parameters (`image_id` vs `ami` respectively).
        - Since they automatically launch new instances as needed, you don't need the `user_data_replace_on_change` parameter previously in the notes!
    - Defining the ASG has a `min_` and `max_size` to have boundaries on how many instances it will create.
    - `launch configurations` have a `lifecycle` parameter which is necessary to allow for updates. Because the `aws_autoscaling_group` references the `launch configuration`, deleting and creating the launch config is not possible.
        - This allows terraform to create a new `launch configuration` first, update the reference within the `aws_autoscaling_group`, then delete the old one.
    - ASGs also need a `subnet_ids` parameter so they know what VPC subnets to deploy the EC2 instances.
- `data` sources allow us to get read-only info from the provider to be used in our code as a reference, kind of like how variables are used! Their syntax is `data "<PROVIDER>_<TYPE>" "<NAME>" { [CONFIG...] }`. `type` in this case refereces to the data source you want to use (eg `vpc`).
    - Elsewhere in code, they can be referenced with the syntax `data.<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>`.
    - Attributes of `data` sources can also be used within other `data` sources, for example getting `aws_subnets` data can use something like `data.aws_vpc.default.id` within its `filter` config block to retrieve subnet information from the default vpc (See code for this in action).
- Elastic Load Balancers (ELBs) allow you to have one resource that as the ingress point for your cluster and distributes traffic across them. This provides your users a single IP/URL to use vs individual IPs for each EC2 instance. There are three types:
    - Application Load Balancer (ALB) - best suited for HTTP(S) traffic.
    - Network Load Balancer (NLB) - best for TCP, UDP, and TLS, and can scale to load faster than an ALB (designed to scale for tens of millions of requests per second).
    - Classic Load Balancer (CLB) - "legacy" load balancer.
- ALBs 
    - They consist of several parts:
        - Listener - specific port and protocol it listens on.
        - Listener Rule - takes requests into the listener and sends them to specific target groups based on paths or hostname matches.
        - Target Groups - One or more servers that receive requests. The Group assess the health of its servers and only sends requests to healthy nodes.
    - They are created by defining a few resources: the ALB itself (`aws_lb` with `application` defined in parameters), the `aws_lb_listener`, the `aws_lb_listener_rule`, the `aws_lb_target_group`, and an `aws_security_group` for the ALB.
    - We can change our `output` for public IP to the `dns_name` of our `aws_lb` resource, so that we now get the singular, public address the ALB will use to front our cluster!
    
## Chapter 3 - How to Manage Terraform State
- Running `terraform apply` records information about the current state of infrastructure in a terraform `state file`.
    - By default, this file is created in your working directory as `terraform.tfstate` and is a JSON file that maps resources in your config to the real world resources in AWS.
    - Running `plan` is essentially diffing your code against the deployed infrastructure.
    - Never edit this file by hand!
    - Some challenges arise when using terraform as a team: how to use shared storage for state files, how to lock state files when someone's using it, and how to isolate state files between environments (dev vs prod).
- **Shared Storage for State Files**
    - The best way to manage shared storage for state files is to use remote `backends`; the built-in way terraform determines how to load and store state.
    - Through chapter 2, we used a `local backend`, a state file saved to local disk. In a distributed team, we would want to use something `remote` and accessible to everyone, such as S3. They solve common problems such as:
        - **Manual error** - the state file will be loaded and then stored again after each `plan` and `apply`, preventing any manual error.
        - **Locking** - many remote backends support some type of locking feature, allowing for one individual to run `apply` and affect changes at a time.
        - **Secrets** - most remote backends support in-transit and at-rest encryption of the state file, as well as IAM policies for acess control. Additionally, using a remote backend prevents storing secrets in plain-text on local disk.
    - S3 offers a number of benefits and will be used through this course.
    - Defining an S3 bucket is similar to other `resources` but a few notes:
        - The `bucket` parameter is the real world name we're giving to the bucket, and this must be *globally unique* to all AWS customers!
        - We can set a `prevent_destory` policy within the `lifecycle` parameter to prevent an `apply` from deleting that resource (terraform will exit with an error).
        - We need to define other resources to enable feaures like versioning, encryption, blocking public access...
    - DynamoDB tables are used for locking the state files while in use.
        - When creating this resource, it *must* have a primary key called `LockID` (exact spelling and capitalization).
    - Terraform requires a `backend` configuration so that it knows to store your state file in your S3 bucket instead of locally on disk!
        - This is done by adding a `terraform` block that configures terraform itself, and uses the syntax `terraform { backend "<BACKEND_NAME>" { [CONFIG...] }}` where `BACKEND_NAME` is the backend we want to use (eg S3). See the code for more detail!
    - After configuring the bucket and `terraform` block to use the bucket for remote state file storage, if we run `terraform init` again, it will then begin to store the state file in the bucket and use in this configuration.
    - Interestingly, we had to write code to create the bucket and DynamoDB table and use a local backend for that state file. Then we wrote code to add a backend for the bucket and table, then reinitialize terraform to use it.
        - The good news this is a one time process, and the rest of our code can use this `backend` configuration from the get go.
    - Note that we also have to manually write the `terraform` backend configuration for every module (learned in later lessons) and can't use variables. We also have to have a unique key (file path within the bucket) for every module we create, so be careful not to just copy and paste the backend config verbatim.
- **State File Isolation**
    - So far through the book, we've written all of our tf code in one file and folder, using one backend config, which stores **all** of our state in a single file. This means one mistake could break our entire infrastructure.
    - There are two ways we can chose to isolate state files, either by `Isolation via workspaces` or `isolation via file layout`.
    - **Isolation via Workspaces**
        - By default, terraform starts with a single workspace called `default`, and this workspace stores state files in the `key` you specify in your backend configuration.
        - You can use `terraform workspace new {workspace_name}` to create new isolated workspace environments, and running `apply` on the same configuration files but in different workspaces will create new resources.
        - Creating workspaces and applying configs creates an `env` folder in your s3 bucket, with child folders for each workspace that contain their own state files.
        - You can switch between resources with `terraform workspace select {workspace_name}`.
        - This is a nice way to test changes on a deployed module without affecting running infrastructure!
        - Drawbacks include:
            - Using the same credentials for all workspaces, which prevents true isolation.
            - Workspaces are not visible in the code, so real world infrastructure may be different than the codebase.
    - **Isolation via File Layout**
        - Full isolation can be achieved by putting config files for different environments (eg prod and dev) in their own folders, and by configuring a different backend for each environment that can have different authentication and access controls.
            - [An example can be found here](https://blog.gruntwork.io/how-to-manage-terraform-state-28f5697e68fa#:~:text=Here%E2%80%99s%20the%20file%20layout%20for%20my%20typical%20Terraform%20project%3A)
            - To take it further, you could also put separate "components" into their own folders like a `mgmt` folder for more static resources (eg VPCs, subnets, etc...) and `services` (eg a frequently modified webapp) so that constant changes in one component won't run the risk of modifying static infrastructure.
        - Terraform doesn't care about filenames but it is useful to follow some convention for your team. Examples would be:
            - `variables.tf` for input vars, `outputs.tf` for output vars, `main.tf` for resources and data sources, etc... You can go further such as `providers.tf` for quick reference of provider information.
        - This layout has some advantages like clear understanding of how components are deployed and in what environments, and there's decent isolation to minimize the blast radius of any mistakes.
            - However a drawback is that now `apply` has to be run in each module individually to create your entire infrastructure.
    - **`terraform_remote_state` Data Source**
        - This data source is useful to allow us to get a state file of other modules to use that information in a module we might be currently working in!
        - For this topic we wil create a MySQL db on AWS RDS. Important notes:
            - The DB creation will require a username and password passed in. Since we want to avoid secrets in plain text, for now we will create variables in `variables.tf` and pass those values in via the command line for example `export TF_VAR_db_username=<username>`
            - We also create output variables about the DB that we want our webserver cluster to take as input so it can connect to the DB. We do this by adding a `data` block to the `main.tf` file for the cluster and pointing it to the statefile for the DB.
            - We also use a `templatefile` to read in data from a bash script, as opposed to inline Bash, that was formerly in the `user_data` parameter of `main.tf`

## Chapter 4 - How to Create Reusable Infrastructure with Terraform Modules
- At the end of chapter 3, we created [this architecture](https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d#:~:text=In%20the%20previous%20post%2C%20you%20deployed%20architecture%20that%20looks%20like%20this) in our `stage` folder, which if we want a `stage` and prod environment, would mean we'd have to copy this code over to the other environment.
    - These environments would be largely identical but with some slight differences, such as possibly having smaller servers in `stage` to save money.
- Like general-purpose programming languages that the idea of reusable *functions*, terraform has `modules` that can be reused throughout your code!
    - For example, if we have a "standard" webserver-cluster in our environments, we can break this out into a module that can then be pulled into the code for each environment and given minor tweaks to fit that environment.
    - This is key to allowing terraform to be reusable, maintainable, and testable!
- **Module Basics**
    - In fact, a `module` in terraform is any set of configuration files within a folder. So in effect, we've actually been creating modules this entire time. But because we're running apply directly on a module, it's considered a `root module`.
        - What we really want to do is create `reusable modules`, those that are *meant* to be used within other modules.
    - Using `stage/services/webserver-cluster`, we're going to explore creating reusable modules.
        - First we want to remove the `provider` definition in `main.tf` as *providers* should be defined in root modules.
        - Modules use the syntax `module "<NAME>" { source = "<SOURCE>" [CONFIG...]} where NAME is how we will refer to this module throughout our code, SOURCE is the path of the module code, and CONFIG is specific arguments
        - Checking the code in this repo, you can see that we've imported the `webserver-cluster` module into both `stage` and a new environment `prod`. Anytime we add a module or modify a module's `source`, we'll want to run `init` before `plan` or `apply`. You can see when we `init` in `prod` for example, one of the console output lines is actually initializing the module we want to import!
- **Module Inputs**
    - Like a programming language's functions can take input variables, so can modules in the form of their parameters. We can also create a `variables.tf` in a module path to define these inputs.
    - At the end of chapter 3, we hardcoded all of the cluster resource names into `main.tf` that we moved to the new module folder, so if we were to run apply in both `stage` and `prod`, we'd have naming conflicts. So we will want to use `var.cluster_name` instead.
    - Since we created these variables in the module's `variables.tf` file, we can pass values in when we import the module in each environment's `main.tf` by passing values for each variable in the module import. In the `module` definition `main.tf` we use `var.variable_name` to pass it back to itself. See the code for how that works!
- **Module Locals**
    - For some values, using variables might lead to issues because their value can be affected elsewhere in the codebase by you or another user. For example, the `cluster_name` variable can change by accident. We might want some values to remain constant, such as the listener port (80) and not be editable, so we can use the concept of `locals`.
    - Locals are only visible within the module and have no impact on others, and they can't be overridden from outside of that individual module. They use the syntax `local.<NAME>`. We will edit some of the network information for our http listener and alb ingress/egress blocks with locals! See the code.
- **Module Outputs**
    - ASGs can be configured to scale up or down on a schedule using `scheduled actions`, which can be a nice feature if you have repeated timeframes of increased traffic.
        - Note that this might not be best defined in a module, since the module is imported into `stage` and `prod`, and scheduled scaling might not be necessary in a staging environment. So far now we will explore this in the prod environment only.
        - When we create an output in the modules `outputs.tf`, they can be accessed in an environment's `outputs.tf` by defining an output and using the `module.<MODULE_NAME>.<OUTPUT_NAME>` syntax.
- **Module Gotchas**
    - We may want to be aware of two things when creating modules: File paths and inline blocks.
    - File Paths - the `user_data.sh` file is now in our modules folder, and terraform's `templatefile` function reads files from the *relative path* on local disk, meaning when we first created the module, neither environment would be able to successfully call it.
        - We can instead user a `path reference` (`path.<TYPE>`) such as `path.module`. In this case it will use the filesystem path of the module *where the expression is defined*.
        - So in our module's `main.tf` where we define the path of our script, we want to prefix with our path type like so: `${path.module}/user_data.sh` so that other environments know this file is located within the module definition path!
    - Inline blocks - some configuration for resources can either be inline block-defined or defined as separate resources. For example, the `ingress` and `egress` definitions we've done for the security group resource can actually be their own separate resources `aws_security_group_rule`.
        - Mixing the two however can lead to conflict errors as they may attempt to overwrite each other, so when creating modules, it may be best to define them as separate resources. This is because the separate resources can be added anywhere whereas the inline block only applies to the module that creates the resource.
        - To further clarify, in our module we originally defined the `ingress` and `egress` in `aws_security_group alb` so users won't be able to add more rules outside of the module.
        - Changing these to their own resources then allows us in an environment `main.tf`, for example in `stage`, to define additional rules if desired when importing the module while retaining the defaults we create in the module `main.tf`
        - Had we left the original inline blocks in the module `main.tf`, creating additional during the module import would have lead to errors in the rules attempting to overwrite one another.
- Important Note: The book, for simplicity, uses on VPC for both `stage` and `prod` environments. This is not recommended or ideal in actual real world production. Misconfigurations in either environment to critical resources (like route tables) could affect traffic to and from the entire VPC affecting all environments.
    - Thus it is recommended to use separate VPCs, or even separate AWS accounts if possible, to achieve isolation between environments.
- **Module Versioning**
    - So far we created one module that is imported into both `stage` and `prod` environments. This causes an issue in that changes to the module will affect both environments, making testing changes in `stage` much more likely to affect production. A better approach would be to use `versioned modules`, with versions applying to separate environments.
    - An easy way to achieve this versioning is to store module code in a separate git repo and setting the `source` parameter of the module imports to that URL.

## Chapter 5 - Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas
As a declarative language certain operations are more difficult like loops, if-statements, etc.. Luckily terraform does provide a few primitives that we will see in this chapter! We can use these in a few different loop constructs in terraform:
### Loops
- **Loops with the `count` parameter** - loops over resources and modules. We will see this in action by creating a number of users with AWS IAM.
    - Every resource has `count` as a parameter. It is quite primitive and all it does on its own is define the number of copies we want to make of that resource.
    - As you can see in the code, we can use `count.index` to append the index to a value for the resource's `name` parameter as count iterates through the value we set for itself (`count = 3 name = "neo.${count.index}"`). In this instance though, each user would be named `neo.` with whatever value of the index was iterated next which may not be particularly useful.
        - Instead, we could create a variable called something like `user_names` with a default value of a list that has 3 separate names. 
        - Since terraform also has arrays (using the `Array lookup syntax`) and a length function, we can combine these with count using like so: `count = length(var.user_names) \n name = var.user_names[count.index]`. So here `count` is set to the length of the list in teh variable, and we iterate through count and use each integer back against the list as its index and the usernames are pulled in from each index.
        - What we've done now is create `aws_iam_user.example` as an array, not a single resource, so it can't be accessed with the usual syntax of `<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>` (a user name as `aws_iam_user.example.name`), instead we also need to use array lookup syntax on it like `<PROVIDER>_<TYPE>.<NAME>[INDEX].<ATTRIBUTE>`. See the `outputs.tf` file for examples!
    - `count` can also be used on modules just like resources. When we import a module, we can add `count` as a parameter (just like we did the with the `aws_iam_user` example), point it to a variable to get an array or list, and use `count.index` to iterate through the list the same way.
    - `count` cannot be used in inline blocks however. Consider the `tag` block of our `aws_autoscaling_group` resource. Unfortunately we cannot write code that allows users to pass in tags during creation by setting up similar logic to our previous examples here.
    - Another limitation of `count` is that, again, resources become arrays, so changing values of a resource can lead to destroying parts of the array. 
        - In our examples above, we had a `user_names` list variable that had 3 usernames within it. Say we created 3 users and user `trinity` was at index 1 (`aws_iam_user.example[1]`). If we decided we wanted to remove trinity from the list, because of her position in the list, terraform will instead rename her to morpheus and delete the original morpheus user.
        - Terraform is essentially deleting every item to the right of trinity in the list and recreating them one index position to the left.
        - The end result *looks* the same, but this can lead to data loss. A
        \gain the morpheus user is not the original user, it has been destroyed and recreated!
- **Loops with `for_each` Expressions** - The `for_each` expression allows us to loop over lists, sets, and maps to create a) multiple copies of an entire resource, b) multiple copies of an inline block within a module, or c) multiple copies a module itself.
    - THe syntax looks like `resource "<PROVIDER>_<TYPE>" "<NAME>" { for_each = <COLLECTION> [CONFIG...]}`
        - `COLLECTION` is a set or map to loop over (lists are not supported in `for_each` on a resource) and `CONFIG` consists of one or more arguments specific to that resource.
            - In `CONFIG` you can use `each.key` and `each.value` for the current item in `COLLECTION`.
        - Look at the code for the example, but when `for_each` loops over the `user_name` *set* it makes each username available under `each.value`, which we assign to `name` for each IAM user resource that is created.
            - `each.key` also contains the username(s) but is typically only used with maps and key-value pairs.
        - Using `for_each` on a resource creates a *map* of resources as opposed to one resource (or an *array* or resources as with `count`). The keys in the map are the usernames (the keys) in `for_each`. Accessing specific information then, like the ARN as before, requires a different sytnax `value = values(aws_iam_user.example)[*].arn` for our `output` variable. See the code.
    - The fact that `for_each` creates maps is a big deal compared to `count`! Because it creates a map, terraform doesn't depend on the indexing of a list if we make changes as before, so we can safely remove an element from the map without the concerns we had before of editing the list! For this reason it may be preferable to choose `for_each` over `count` to create multiple copies of a `resource`.
    - `for_each` also has the advantage of creating multiple inline blocks withn a resource. We will see how to use it to generate dynamic `tag` inline blocks for the ASG in the `webserver-cluster` module.
        - First we create a new map input variable that allows users to specify custom tags in `variables.tf` of the module.
        - Next we can try setting custom tags in the `prod` `webserver-cluster` module import by defining a `custom_tags` block that has dynamic key/value tags.
            - These can be updated at any point and terraform will make those changes in place if we desire.
        - Finally we add a `dynamic` block to the ASG resource in `main.tf` for the `webserver-cluster` within `modules`. The syntax is `dynamic "<VAR_NAME>" { for_each = <COLLECTION> content{[CONFIG...]}}`:
            - `<VAR_NAME>` is what we want to pass into the instance as the variable name (in this case "tag").
            - `COLLECTION` is the list or map of variables we want to iterate over (in this case `var.custom_tags`).
        - What is happening here is that for each instance that is created by the ASG, create dynamic variables (module sees `var.custom_tags` in the module's `variables.tf`) for the keys we specified during the module import within `main.tf` in the `prod` module block of `webserver-cluster`.
            - We specified `Owner` and `ManagedBy` with their own respective values.
    - *Note on tags* - it is best practice to come up with a tagging standard for your team, which can be tedious to enforce if you are using the above method to set "globally-default" tags on every resource in your entire AWS environment.
        - For these types of tags, you can define a `default_tags` block within your `provider` block to ensure that all resources will have these minimal default tags applied!
- **Loops with the `for` String Directive** - We've previously see string interpolations allowing us to reference variables within strings, like "Hello, ${`var.name`}". `String directives` allow us to use control statements (for-loops and if-statements) within a string and use a similar, but different, syntax.
    - That syntax is `%{ for <ITEM> in <COLLECTION> }<BODY>%{ endfor }`. Note that here we use a `%` and not a `$` like in string interpolation.
        - `COLLECTION` is a list or map to loop over, `ITEM` is the local var name to assign to each item in `COLLECTION`, and `BODY` is what to render.
### Conditionals
Terraform offers several different ways to do conditionals, just like with loops.
- **Conditionals with the `count` Parameter**
    - **If-statements with the count parameter**
        - Terraform supports *conditional expressions* in *ternary syntax* to combine `if` statements with our `count` parameter to achieve things like applying `autoscaling` to prod resources and not to stage. The syntax is `<CONDITION> ? <TRUE_VAL> : <FALSE_VAL>`
        - First we define a boolean variable in the `variables.tf` for our cluster in `modules` called `enable_autoscaling`.
        - Next we define `aws_autoscaling_schedule` in the module's `main.tf` similar to how we previously defined it directly in the `prod` import, but here we add a first line of `count = var.enable_autoscaling ? 1 : 0`.
            - In the `module` import block of each `main.tf` in prod and stage, we can then set `enable_autoscaling` to either true or false. If true, then the `aws_autoscaling_schedule` resources we defined in the module's `main.tf` will be created!
    - **If-else statements with the `count` parameter**
        - The book has a self-admitted contrived example for demonstrating this, but it does give good insight into how this might work, using IAM policies and attaching them based on boolean logic.
            - The example uses two `aws_iam_policy` resources, one read-only and one full access, and two associated `data` blocks that each pulls the policy rules from.
            - Then we create a boolean variable for granting the user one policy or the other.
            - Then we create two `aws_iam_user_policy_attachment` resources, each one with their own `count` parameter.
                - The interesting part is that the policy attachment for full access has a `count` parameter of `count = var.give_full_access ? 1 : 0` while the read-only attachhment has `count = var.give_full_access ? 0 : 1`.
                - So if the variable we're referencing is set to `true`, then the user gets the full access policy attachment applied, or will get read-only applied if its `false`.
                - This inverse logic is how we can do an `if-else` clause in terraform!
- **Conditions with `for_each` and `for` Expressions**
    - We can improve our original dynamic tag logic from earlier in the chapter by adding some `for_each` and `for` logic to it.
    - In this example we `for` logic to the loop to use uppercase on the value of each tag (maybe for consistency), but to also skip any keys set to "Name", because we've already defined it in the block above.
    - The book recommends using `count` to conditionally create resources and modules, and using `for_each` for other types of loops and conditionals.
- **Conditionals with the `if` String Directive**
    - I read through this portion but did not add the code, simply because at the moment I was not interested in outputting the users list, however this is a good topic to review to manipulate, or further refine, how we can use conditionals in string directives to format how our output will be presented in the terminal (or referenced by other resources)!
### Zero Downtime Deployment
So far our work has been to build a clean and simple API for deploying our cluster, and we have not dealt with how we might want to push updates of our code (our simple "Hello, World!" website)/deploy a new AMI across the cluster; and certainly have not explored how to do that with zero downtime.
- In real world production, our webserver code probably live within the AMI itself, so we wouldn't necessarily be deploying new images. But in our examples throughout the book, all of that data is in the `user_data.sh` script, and the AMI is a vanilla Ubuntu image, so we'll make some changes to illustrate this process!
    - To do this, we will define a default AMI in `variables.tf` for the cluster module, as well as a default string for the webserver to return, as their own variables.
    - Since we're defining the default text in the variable, we need to edit `user_data.sh` to use that variable. We will also edit the `image_id` parameter within the `aws_launch_configuration` resource definition to use the `var.ami`, and add the `server_text = var.server_text` logic into `user_data`'s templatefile call.
    - If we now use `main.tf` in `stage` to illustrate this, and we add parameters for `ami` and `server_text` for the module import to consume, we will see that running `plan` shows us it wants to replace the old launch config with a new one that has the updated `user_data`, and it wants to modify the ASG to reference that new launch config.
    - We now have the issue that this reference will not do anything until we instruct the ASG to launch new instances. We could have terraform destroy the old ASG and launch a new one, but users will experience downtime. Using the `create_before_destroy` lifecycle setting though can let us spin up a replacement **before** destroying the old ASG!
        - First we can configure the ASG `name` parameter to depend on the name of the launch configuration (so when the launch config name changes, terraform will see that and force a replacement of the ASG).
        - Set `create_before_destroy` to `true` for the ASG.
        - Set `min_elb_capacity` of the ASG to the `min_size` of the cluster so that terraform will wait for at least that many new servers in the new ASG to be created before passing health checks in the ALB and destroying the original ASG.
        - A nice feature here is that during the rollout of the new ASG, if there is any issues preventing the new instances of launching, terraform will roll back to the last version after a `wait_for_capacity_timeout` (default is 10 minutes) of the `min_elb_capacity` we set to register with the ALB.
- **Coming back to this chapter after fixing some issues with my configs**

## Chapter 6 - Managing Secrets with Terraform
- **Secret Management Tools**
    - **Types of secrets you store** - it's important to understand the different types of secrets you are storing, as that will determine how best to do that.
        - The way you'd store say personal secrets is different than infrastructure secrets, because the use case is different. For instance, you may need to frequently decrypt an infrastructure secret for use, so that might be stored with some reversible encryption method. Whereas storing customer secrets might just be a salted hash as the secrets are just meant to be stored and not used.
        - The book offers further explanations of methods to store secrets and a comparison chart of use cases and possible providers.
- **Secret Management Tools with Terraform**
    - **Providers** - working with providers requires the use of secrets to authenticate to that provider. For instance, running `apply` against the AWS provider requires the use of your access key.
        - **Human Users** - reference the book or blog for more info on this, but the author generally compares and contrasts hardcoding in provider definition (never do), to setting environment variables, to other CLI tools you can use.
        - **Machine Users** - how you might want to setup an autonomous system (for example a CI server) to authenticate with a provider to run your code depends on from and to what machines you're completing this workflow.
            - *CircleCI with stored secrets* - This example uses CircleCI to illustrate how you can define the workflow for your pipeline to run terraform commands with your code, using credentials you create for a "machine user" and store within the CircleCI's secrets manager, CircleCI Context. At runtime, Context exposes the secrets to that server in the form of environment variables.
            - *EC2 with Jenkins as a CI server, with IAM roles* - using an EC2 instance to run terraform (eg Jenkins on EC2 as a CI server), a solution is to use an `IAM role`.
                - See the blog or book for an example of setting up the role and policy resources, data sources, etc...
                - Note that all EC2 instances expose an *instance metadata endpoint* on every EC2 instance, and instances with IAM roles attached will include AWS credentials. Tools like Terraform that use the AWS SDK know how to use those endpoint credentials when running commands like `apply`.
                - These creds are always temporary and rotated automatically.
            - *Github Actions as a CI Server with OIDC* - OIDC allows third-party CI tools like Github Actions to authenticate to your cloud provider without any manual management of credentials (like in the CircleCI example).
                - See the book for more detailed example.
    - **Resources and Data Sources** - recall the example we've seen throughout the book of `aws_db_instance` needing the preconfigured username and password. Remember, never put these credentials in your code in plain text!
        - **Environment Variables** - one method we can use is pass environment variables at `plan` or `apply` time.
            - Create `variable` definitions for `db_username` and `db_password`.
            - In the resource definition for the db, we can pass `var.db_[username/password]` as values, and in our CLI `export TF_VAR_db_[username/password]=(actual values)`
            - Drawbacks to this method include a) not everything is defined in terraform (there's the extra step of setting our local environment variables), b) there's no standard for how people save the secrets themselves, and c) the secrets are not versioned or tested, so configuration errors between environments can happen.
        - **Encrypted Files** - as mentioned before, we can use an encrypted file checked in to version control to retrieve necessary secrets.
            - See the book for detailed example but in essence, we can leverage AWS KMS with terraform to defnie a *Customer Managed Key* (CMK) to encrypt and decrypt the file.
            - The key is never exposed to us as local users, and permissions for the key can be granted to the current user.
            - Note that if you want to change information in the encrypted file, it does require manually decryption and re-encryption.
            - Drawbacks to this approach can include more manual commands, more integrations required with automated tests, rotating and revoking secrets is hard (everything's checked in), auditing can be difficult (we can see who used KMS but not what for), etc...
        - **Secret Stores**


